import os
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
#CONFIG
from dotenv import load_dotenv
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=GOOGLE_API_KEY)

#FLASK APP
app = Flask(__name__)
CORS(app)

#LOAD DATA
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data", "Data.txt")
INDEX_PATH = os.path.join(BASE_DIR, "index")

with open(DATA_PATH, "r", encoding="utf-8") as f:
    raw_text = f.read()

#SPLIT & FILTER
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_text(raw_text)
docs = [d for d in docs if d and d.strip()]  # lo·∫°i b·ªè None ho·∫∑c r·ªóng

if not docs:
    raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu")

# EMBEDDING MODEL
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-MiniLM-L3-v2",#sentence-transformers/all-MiniLM-L6-v2
    model_kwargs={"device": "cpu"}  #cuda
)

# EMBEDDING MODEL (ch·ªâ khi c·∫ßn build m·ªõi kh·ªüi t·∫°o)
embedding_model = None

if not os.path.exists(INDEX_PATH):
    print("‚ö° Ch∆∞a c√≥ index ‚Üí T·∫°o m·ªõi...")
    from langchain_community.embeddings import HuggingFaceEmbeddings
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-MiniLM-L3-v2",
        model_kwargs={"device": "cpu"}
    )
    vectorstore = FAISS.from_texts(docs, embedding_model)
    vectorstore.save_local(INDEX_PATH)
else:
    print("‚úÖ ƒê√£ c√≥ index ‚Üí Load nhanh...")
    from langchain_community.embeddings import HuggingFaceEmbeddings
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-MiniLM-L3-v2",
        model_kwargs={"device": "cpu"}
    )
    vectorstore = FAISS.load_local(
        INDEX_PATH,
        embedding_model,
        allow_dangerous_deserialization=True
    )

retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
# ==================== RAG CHAT FUNCTION ====================
def rag_chat(query: str):
    if not query:
        return "C√¢u h·ªèi r·ªóng."
    try:
        docs = retriever.get_relevant_documents(query)
        context = "\n".join([d.page_content for d in docs])  # truncate
        prompt = f"""B·∫°n l√† chatbot Tosi h·ªó tr·ª£ Aquaponics.
Gi·ªõi thi·ªáu b·∫£n th√¢n khi ng∆∞·ªùi d√πng ch√†o.
Tr·∫£ l·ªùi tr·ª±c ti·∫øp, kh√¥ng ch√†o h·ªèi d∆∞ th·ª´a.
Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† chi ti·∫øt d·ª±a tr√™n ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y.
T·ªïng h·ª£p t·∫•t c·∫£ th√¥ng tin li√™n quan, kh√¥ng b·ªè s√≥t.
N·∫øu li·ªát k√™, m·ªói √Ω m·ªôt d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng '-'.
N·∫øu kh√¥ng c√≥ th√¥ng tin, tr·∫£ l·ªùi: "Xin l·ªói. ƒêi·ªÅu n√†y kh√¥ng c√≥ trong t√†i li·ªáu"


Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi: {query}
"""
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(prompt, generation_config={"temperature":0.2})
        return response.text if response and response.text else "Xin l·ªói, kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi."
    except Exception as e:
        print(" L·ªói rag_chat:", e)
        return " Bot g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi."

#ROUTES
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    try:
        data = request.json
        query = data.get("query", "")
        answer = rag_chat(query)
        return jsonify({"response": answer})
    except Exception as e:
        print("L·ªói /chat:", e)
        return jsonify({"response": f" Server error: {str(e)}"})

#RUN
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))  # Render s·∫Ω truy·ªÅn PORT, local m·∫∑c ƒë·ªãnh 5000
    print(f"üöÄ Starting Flask app on port {port}")
    app.run(host="0.0.0.0", port=port, debug=True)